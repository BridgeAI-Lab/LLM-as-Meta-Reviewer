{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ex7Gph9-UrV-",
   "metadata": {
    "id": "ex7Gph9-UrV-"
   },
   "source": [
    "#Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd385f",
   "metadata": {
    "id": "6cfd385f"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json, ast, time\n",
    "import sys\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import pearsonr, spearmanr,kendalltau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "data_path = \"<Path of the Excel Files>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f38f3",
   "metadata": {
    "id": "725f38f3"
   },
   "source": [
    "#Reading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148ea7e",
   "metadata": {
    "id": "4148ea7e"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "LLM1- PalM2\n",
    "LLM2- GPT3.5\n",
    "LLM3- Llama2\n",
    "'''\n",
    "# do the same for all LLMs\n",
    "data = pd.read_excel(data_path+\"llm1_dataset.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6YusivpwX1vq",
   "metadata": {
    "id": "6YusivpwX1vq"
   },
   "source": [
    "#GPT Evaluation (Macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xJ4hG0vvpI1M",
   "metadata": {
    "id": "xJ4hG0vvpI1M"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VmwMw1VbpI1M",
   "metadata": {
    "id": "VmwMw1VbpI1M"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# openai.api_key =\"<OPENAI-KEY>\"  uncomment it\n",
    "\n",
    "\n",
    "def get_gpt_output(inp, role='user', temp=0.0):\n",
    "    message = [{'role': role, 'content': inp}]\n",
    "    fail_count = 0\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            completion = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=message,\n",
    "                temperature=temp,\n",
    "            )\n",
    "            break\n",
    "        except openai.error.RateLimitError as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "        except openai.error.APIError as e:\n",
    "            print(e)\n",
    "            fail_count += 1\n",
    "            if fail_count > 10:\n",
    "                raise e\n",
    "            time.sleep(5)\n",
    "        except openai.error.ServiceUnavailableError as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "        except openai.error.TimeoutError as e:\n",
    "            print(e)\n",
    "            time.sleep(5)\n",
    "\n",
    "    reply_content = completion.choices[0].message.content\n",
    "#     print('Reply: ', reply_content)\n",
    "\n",
    "    return reply_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vJB7iZ70pI1M",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "executionInfo": {
     "elapsed": 1612,
     "status": "ok",
     "timestamp": 1725329712929,
     "user": {
      "displayName": "NLP Research",
      "userId": "12472206343781890824"
     },
     "user_tz": 240
    },
    "id": "vJB7iZ70pI1M",
    "outputId": "6b082bb7-1acc-4aa0-9626-5a4b08c740c5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Sure! I'm an AI language model created by OpenAI, designed to assist with a wide range of tasks by generating human-like text based on the input I receive. I can help answer questions, provide explanations, generate creative content, and much more. My knowledge is based on a diverse range of sources up until my last update in October 2023. How can I assist you today?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpt_output(\"Tell me about yourself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9XN1-oAvzlEk",
   "metadata": {
    "id": "9XN1-oAvzlEk"
   },
   "outputs": [],
   "source": [
    "## TELR Prompts Level 1 to 4\n",
    "\n",
    "prompt_1 = f\"\"\"\n",
    "        Using 3 reviews given below draft a meta review:\n",
    "        1.Review#1\\n< {r1} >\\n\n",
    "        2.Review#2\\n< {r2} >\\n\n",
    "        3.Review#3\\n< {r3} >\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "prompt_2 = f\"\"\"\n",
    "        Using the three reviews, generate a meta review by incorporat-\n",
    "        ing core contributions, common strengths, common weaknesses,\n",
    "        common suggestions for improvement, and missing references.\n",
    "        Common strengths, Common weaknesses, and Common sugges-\n",
    "        tions are strengths, weaknesses, and suggestion for improvement\n",
    "        respectively that are common in at least 2 reviews. Three reviews\n",
    "        are as follows:\\n\n",
    "        1.Review#1\\n< {r1} >\\n\n",
    "        2.Review#2\\n< {r2} >\\n\n",
    "        3.Review#3\\n< {r3} >\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "prompt_3 = f\"\"\"\n",
    "      You are a meta-review assistant. Using three reviews given as\n",
    "      Review#1, Review#2, and Review#3, give me a meta-review by\n",
    "      answering:\n",
    "      (a) Mention core contributions with common contributions\n",
    "      first.\n",
    "      (b) A common strength is a strength that is mentioned in at\n",
    "      least 2 reviews as a strength. Mention common strengths.\n",
    "      (c) A common weakness is a weakness that is mentioned\n",
    "      in at least 2 reviews as a weakness. Mention common\n",
    "      weaknesses.\n",
    "      (d) A common suggestion for improvement is a suggestion\n",
    "      that is mentioned in at least 2 reviews as a suggestion for\n",
    "      improvement. Mention common improvements suggested.\n",
    "      (e) State whether reviews refer to missing references or not.\n",
    "      A listing of missing references is not required.\n",
    "      Three reviews are as follows:\\n\n",
    "      1.Review#1\\n< {r1}>\\n\n",
    "      2.Review#2\\n< {r2} >\\n\n",
    "      3.Review#3\\n< {r3} >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt_4 = f\"\"\"\n",
    "      Using three reviews given as Review#1, Review#2, and Review#3,\n",
    "      as a meta-reviewer, your task is to draft a meta review\n",
    "      by answering the following bulleted questions:\n",
    "      - What is the summary of core contributions? Provide answer\n",
    "      with supporting evidence.\n",
    "      - Which common strengths are referred to in the reviews? A\n",
    "      common strength is a strength that is mentioned in at least 2\n",
    "      reviews as a strength. Support your answer with explanation.\n",
    "      - What common weaknesses are described in the reviews? A\n",
    "      common weakness is a weakness that is mentioned in at least 2\n",
    "      reviews as a weakness. Give evidence in support of the reply.\n",
    "      - What suggestions for improvement have been provided by\n",
    "      three reviews? A common suggestion for improvement is a\n",
    "      suggestion that is mentioned in at least 2 reviews as a suggestion\n",
    "      for improvement. Explain the basis for the answer.\n",
    "      - Do the reviews mention about missing references? Answer with\n",
    "      explanation is desirable but listing of missing references is not\n",
    "      required.\n",
    "      Reviews are as below:\\n\n",
    "      1.Review#1\\n< {r1} >\\n\n",
    "      2.Review#2\\n< {r2} >\\n\n",
    "      3.Review#3\\n< {r3} >\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompts = [prompt_1,prompt_2,prompt_3,prompt_4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xJf8HcnGX4nE",
   "metadata": {
    "id": "xJf8HcnGX4nE"
   },
   "outputs": [],
   "source": [
    "#Here, we will generate the ratings for a particular LLM\n",
    "\n",
    "all_dataframes = list()\n",
    "\n",
    "for p in range(1,5):\n",
    "    # time.sleep(5)\n",
    "    ratings = list()\n",
    "    for i in range(len(data)):\n",
    "        r1 = data['Review1'][i]\n",
    "        r2 = data['Review2'][i]\n",
    "        r3 = data['Review3'][i]\n",
    "        gmr = data[f'res_prompt{p}'][i]\n",
    "        amr = data['Meta_Review'][i]\n",
    "\n",
    "        prompt = prompts[p-1]\n",
    "\n",
    "        evaluation_prompt = f\"\"\"\n",
    "        Imagine you are a human annotator and you are tasked with evaluating the performance of a model in generating a meta-review based on three individual reviews. You are provided with the following:\n",
    "\n",
    "        - The **prompt** that guided the generation of the meta-review.\n",
    "        - The **three individual reviews** (r1, r2, r3) which the model used to generate the meta-review.\n",
    "        - The **generated meta-review** (gmr) created by the model.\n",
    "        - The **actual expert-written meta-review** (amr) that serves as a reference for what the meta-review should ideally look like.\n",
    "\n",
    "        Please carefully examine all of this information and evaluate the model's performance based on the following three criteria. Provide a score on a Likert scale from 1 (poor) to 5 (excellent) for each aspect:\n",
    "\n",
    "        1. **Adherence to instructions**: How well did the model follow the specific instructions given in the prompt? Consider how accurately the model addressed the tasks or questions posed in the prompt when generating the meta-review.\n",
    "          - Score: [1-5]\n",
    "\n",
    "        2. **Ability to create useful Meta-Reviews **: Evaluate the usefulness of the generated meta-review in terms of its practicality and effectiveness for someone preparing a comprehensive meta-review. Consider whether the generated content helps in synthesizing the individual reviews into a coherent and insightful summary.\n",
    "          - Score: [1-5]\n",
    "\n",
    "        3. **Matching against actual expert-written meta-reviews**: Assess the extent to which the model-generated meta-review aligns with the expert-written meta-review. This involves comparing the content, tone, insights, and overall quality.\n",
    "          - Score: [1-5]\n",
    "\n",
    "        ### Input\n",
    "        **Prompt:**\n",
    "        {prompt}\n",
    "\n",
    "        **Reviews:**\n",
    "        - Review 1: {r1}\n",
    "        - Review 2: {r2}\n",
    "        - Review 3: {r3}\n",
    "\n",
    "        **Generated Meta-review:**\n",
    "        {gmr}\n",
    "\n",
    "        **Actual Meta-review:**\n",
    "        {amr}\n",
    "\n",
    "        ### Output\n",
    "\n",
    "        ### Please provide your evaluation in numerical format in a list:\n",
    "\n",
    "        [Adherence to instructions,Ability to create useful MPSs, Matching against actual expert-written meta-reviews ]\n",
    "\n",
    "\n",
    "        Please dont include any other comments or explanations.\n",
    "        \"\"\"\n",
    "\n",
    "        response = get_gpt_output(evaluation_prompt)\n",
    "        convert_list = ast.literal_eval(response) # convert generated response into dictionary\n",
    "        ratings.append(convert_list) # add it to qa list\n",
    "        print(f\"prompt:{p}\", f\"paper: {i+1}\", response)\n",
    "    d = pd.DataFrame(ratings)\n",
    "    d.columns = [f'LLM3P{p}[S1]',f'LLM3P{p}[S2]',f'LLM3P{p}[S3]']\n",
    "    all_dataframes.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_AvE4ZhB5qf5",
   "metadata": {
    "id": "_AvE4ZhB5qf5"
   },
   "outputs": [],
   "source": [
    "final_df = pd.concat(all_dataframes,axis = 1, ignore_index=False)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5GnKNKGs6y_i",
   "metadata": {
    "id": "5GnKNKGs6y_i"
   },
   "outputs": [],
   "source": [
    "## change the sheet name with the LLM that are currently evaluating\n",
    "final_df.to_excel(data_path+'Macro_GPT_score.xlsx',sheet_name = 'palm2', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VUKt_dKG71vH",
   "metadata": {
    "id": "VUKt_dKG71vH"
   },
   "outputs": [],
   "source": [
    "# file_name = data_path+'Macro_GPT_score.xlsx'\n",
    "# # Add a new sheet to an existing Excel file\n",
    "# with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "#     # Save the modified DataFrame to a new sheet\n",
    "#     final_df.to_excel(writer, sheet_name = 'llama2', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UpjuG2R-RdBS",
   "metadata": {
    "id": "UpjuG2R-RdBS"
   },
   "source": [
    "## Normalizing Scores for Prompt Levels\n",
    "\n",
    "We have human annotation for four aggregrated samples, therefore we need to average the four paper scores. S1, S2, S3 represents the Statement 1, Statement 2, and Statement 3 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GDGnHWumI2rQ",
   "metadata": {
    "id": "GDGnHWumI2rQ"
   },
   "outputs": [],
   "source": [
    "score = pd.read_excel(data_path+'Macro_GPT_score.xlsx',sheet_name = 'llama2')\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gygbGYlaJV7d",
   "metadata": {
    "id": "gygbGYlaJV7d"
   },
   "outputs": [],
   "source": [
    "# Initialize a new DataFrame to store the averaged values\n",
    "new_score = pd.DataFrame()\n",
    "\n",
    "# Loop through each column in the DataFrame\n",
    "for column in score.columns:\n",
    "    averaged_values = []\n",
    "\n",
    "    # Calculate the mean of every set of four rows for the current column\n",
    "    for i in range(0, 40, 4):\n",
    "        avg_value = score[column].iloc[i:i+4].mean()\n",
    "        averaged_values.append(avg_value)\n",
    "\n",
    "    # Add the averaged values as a new column in the new DataFrame\n",
    "    new_score[column] = averaged_values\n",
    "\n",
    "# new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I_YPTJUENjMa",
   "metadata": {
    "id": "I_YPTJUENjMa"
   },
   "outputs": [],
   "source": [
    "# Custom rounding function\n",
    "def custom_round(value):\n",
    "    # Check if the decimal part is 0.50 or greater\n",
    "    if value % 1 > 0.5:\n",
    "        return np.ceil(value)\n",
    "    else:\n",
    "        return np.floor(value)\n",
    "\n",
    "# Apply custom rounding function to the DataFrame\n",
    "new_score = new_score.applymap(custom_round)\n",
    "# new_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dvwekVklKMUc",
   "metadata": {
    "id": "dvwekVklKMUc"
   },
   "outputs": [],
   "source": [
    "# Extract suffixes and create a dictionary to store column names by suffix\n",
    "suffix_groups = {}\n",
    "for col in new_score.columns:\n",
    "    # Extract suffix from column name\n",
    "    suffix = col.split('[')[-1].strip(']')\n",
    "    if suffix not in suffix_groups:\n",
    "        suffix_groups[suffix] = []\n",
    "    suffix_groups[suffix].append(col)\n",
    "\n",
    "# Initialize a new DataFrame to store the averaged values\n",
    "final_score = pd.DataFrame()\n",
    "\n",
    "# Compute the row-wise mean for each suffix group and add to the new DataFrame\n",
    "for suffix, columns in suffix_groups.items():\n",
    "    final_score[f'LLM1[{suffix}]'] = new_score[columns].mean(axis=1)\n",
    "\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SVLMG_f9PZ4W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 380,
     "status": "ok",
     "timestamp": 1725409943575,
     "user": {
      "displayName": "NLP Research",
      "userId": "12472206343781890824"
     },
     "user_tz": 240
    },
    "id": "SVLMG_f9PZ4W",
    "outputId": "529da736-2e91-4348-fd63-63f729121c6f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"final_score\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"LLM1[S1]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31622776601683794,\n        \"min\": 3.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LLM1[S2]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5163977794943223,\n        \"min\": 3.0,\n        \"max\": 4.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.0,\n          4.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LLM1[S3]\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5163977794943223,\n        \"min\": 2.0,\n        \"max\": 3.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2.0,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "final_score"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-78e88baf-504b-4653-8298-6f8c4aaee9d6\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LLM1[S1]</th>\n",
       "      <th>LLM1[S2]</th>\n",
       "      <th>LLM1[S3]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-78e88baf-504b-4653-8298-6f8c4aaee9d6')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-78e88baf-504b-4653-8298-6f8c4aaee9d6 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-78e88baf-504b-4653-8298-6f8c4aaee9d6');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-314e0786-098c-4916-847c-e72afbc135ff\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-314e0786-098c-4916-847c-e72afbc135ff')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-314e0786-098c-4916-847c-e72afbc135ff button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "  <div id=\"id_4a4ea644-7003-496d-b892-81413a0994ba\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('final_score')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_4a4ea644-7003-496d-b892-81413a0994ba button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('final_score');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   LLM1[S1]  LLM1[S2]  LLM1[S3]\n",
       "0       4.0       4.0       3.0\n",
       "1       4.0       3.0       2.0\n",
       "2       4.0       3.0       2.0\n",
       "3       4.0       4.0       3.0\n",
       "4       3.0       3.0       2.0\n",
       "5       4.0       4.0       3.0\n",
       "6       4.0       4.0       3.0\n",
       "7       4.0       4.0       3.0\n",
       "8       4.0       3.0       2.0\n",
       "9       4.0       4.0       3.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom rounding function\n",
    "def custom_round(value):\n",
    "    return np.floor(value)\n",
    "\n",
    "# Apply custom rounding function to the DataFrame\n",
    "final_score = final_score.applymap(custom_round)\n",
    "final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DobLyzbmQ7Rf",
   "metadata": {
    "id": "DobLyzbmQ7Rf"
   },
   "outputs": [],
   "source": [
    "file_name = data_path+'Macro_GPT_score.xlsx'\n",
    "# Add a new sheet to an existing Excel file\n",
    "with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "    # Save the modified DataFrame to a new sheet\n",
    "    final_score.to_excel(writer, sheet_name = 'llama2_step1', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W714bCdCU403",
   "metadata": {
    "id": "W714bCdCU403"
   },
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oS6VSK1_U7r1",
   "metadata": {
    "id": "oS6VSK1_U7r1"
   },
   "outputs": [],
   "source": [
    "# Load the human vs averaged GPT4 evaluation\n",
    "# the human and GPT-4 annoation were merged into the step2 file mannually.\n",
    "palm2_score = pd.read_excel(data_path+'Macro_GPT_score.xlsx',sheet_name = 'palm2_step2')\n",
    "gpt_score = pd.read_excel(data_path+'Macro_GPT_score.xlsx',sheet_name = 'gpt3.5_step2')\n",
    "llama2_score = pd.read_excel(data_path+'Macro_GPT_score.xlsx',sheet_name = 'llama2_step2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63Fha2qUVl6w",
   "metadata": {
    "id": "63Fha2qUVl6w"
   },
   "outputs": [],
   "source": [
    "def get_correlation(new_df):\n",
    "  # Separate human and automatic evaluations\n",
    "  human_scores = new_df[new_df['Eval_Type'] == 'Human'].drop(columns=['Eval_Type', 'Paper#'])\n",
    "  auto_scores = new_df[new_df['Eval_Type'] == 'GPT'].drop(columns=['Eval_Type', 'Paper#'])\n",
    "\n",
    "  # Ensure the order of papers is the same\n",
    "  assert all(new_df[new_df['Eval_Type'] == 'Human']['Paper#'].values == new_df[new_df['Eval_Type'] == 'GPT']['Paper#'].values)\n",
    "\n",
    "  # Calculate correlation for each aspect\n",
    "  correlations_aspect = {'Pearson': [], 'Spearman': [], 'Kendall': []}\n",
    "\n",
    "  # Calculate correlation for each aspect\n",
    "  correlations = {}\n",
    "  for column in human_scores.columns:\n",
    "      # print(column)\n",
    "      human_column_scores = human_scores[column]\n",
    "      auto_column_scores = auto_scores[column]\n",
    "      pearson_corr, _ = pearsonr(human_column_scores, auto_column_scores)\n",
    "      spearman_corr, _ = spearmanr(human_column_scores, auto_column_scores)\n",
    "      kendall_corr, _ = kendalltau(human_column_scores, auto_column_scores)\n",
    "\n",
    "      correlations[column] = {\n",
    "          'Pearson': pearson_corr,\n",
    "          'Spearman': spearman_corr,\n",
    "          'Kendall': kendall_corr\n",
    "      }\n",
    "\n",
    "  # Convert to DataFrame for better visualization\n",
    "  correlation_df = pd.DataFrame(correlations).T\n",
    "  correlation_df.columns = ['Pearson', 'Spearman', 'Kendall']\n",
    "\n",
    "  return correlation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ixZ4bcNqgOf1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1725411026931,
     "user": {
      "displayName": "NLP Research",
      "userId": "12472206343781890824"
     },
     "user_tz": 240
    },
    "id": "ixZ4bcNqgOf1",
    "outputId": "373608b0-eb51-4a82-c387-03163e6c9201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pearson  Spearman   Kendall\n",
      "LLM1[S1]  0.102062  0.102062  0.102062\n",
      "LLM1[S2]  0.356348  0.356348  0.356348\n",
      "LLM1[S3] -0.128037 -0.094491 -0.089803\n"
     ]
    }
   ],
   "source": [
    "# Correlation for PalM2\n",
    "palm2_crr = get_correlation(palm2_score)\n",
    "print(palm2_crr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bgWr7KRigUzM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1725411031154,
     "user": {
      "displayName": "NLP Research",
      "userId": "12472206343781890824"
     },
     "user_tz": 240
    },
    "id": "bgWr7KRigUzM",
    "outputId": "8693cf76-8a37-418c-ae9b-32158eab0420"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Pearson  Spearman   Kendall\n",
      "LLM1[S1]      NaN       NaN       NaN\n",
      "LLM1[S2]   -0.212 -0.185695 -0.174078\n",
      "LLM1[S3]    0.000  0.062770  0.058926\n"
     ]
    }
   ],
   "source": [
    "# Correlation for GPT-3.5\n",
    "gpt_crr = get_correlation(gpt_score)\n",
    "print(gpt_crr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DG1z8-q7gXCD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1725411041192,
     "user": {
      "displayName": "NLP Research",
      "userId": "12472206343781890824"
     },
     "user_tz": 240
    },
    "id": "DG1z8-q7gXCD",
    "outputId": "6387bc35-92f7-48ff-ae55-afce5ef330d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Pearson  Spearman   Kendall\n",
      "LLM1[S1]  0.534522  0.496904  0.471405\n",
      "LLM1[S2]  0.442807  0.395285  0.383065\n",
      "LLM1[S3]  0.263523  0.263523  0.248734\n"
     ]
    }
   ],
   "source": [
    "# Correlation for Llama2\n",
    "llama2_crr = get_correlation(llama2_score)\n",
    "print(llama2_crr)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
